{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZH8auNhSfre",
        "outputId": "1c95d7fc-b995-4eeb-8534-9bf084a824a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 59.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 66.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "tCIMih9NU0zI",
        "outputId": "5d5bd9d0-7d89-4a31-feb6-3ba3a88fde32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading data for Multifaceted Abstraction...\n",
            "Generating splits...\n",
            "positive examples pre-trim: 10485\n",
            "negative examples pre-trim: 368828\n",
            "13682\n",
            "positive training examples post-trim: 10449\n",
            "positive validation examples post-trim: 36\n",
            "negative training examples post-trim: 10449\n",
            "negative validation examples post-trim: 3233\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "Tokenizing training data...\n",
            "Tokenizing validation data...\n",
            "Preparing to train...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6535' max='6535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6535/6535 1:04:46, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.217900</td>\n",
              "      <td>0.121207</td>\n",
              "      <td>0.961456</td>\n",
              "      <td>0.203947</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.329787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.145800</td>\n",
              "      <td>0.107130</td>\n",
              "      <td>0.967880</td>\n",
              "      <td>0.244444</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.385965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.120200</td>\n",
              "      <td>0.095687</td>\n",
              "      <td>0.975222</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.448980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.102500</td>\n",
              "      <td>0.077219</td>\n",
              "      <td>0.981646</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.523810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.086300</td>\n",
              "      <td>0.131954</td>\n",
              "      <td>0.972775</td>\n",
              "      <td>0.277311</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.425806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [205/205 00:34]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.07721885293722153, 'eval_accuracy': 0.9816457632303457, 'eval_precision': 0.36666666666666664, 'eval_recall': 0.9166666666666666, 'eval_f1': 0.5238095238095238, 'eval_runtime': 34.5456, 'eval_samples_per_second': 94.629, 'epoch': 5.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 931402240}\n"
          ]
        }
      ],
      "source": [
        "import json\r\n",
        "import torch\r\n",
        "import random\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import gc\r\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding\r\n",
        "\r\n",
        "data_directory = '/content/drive/MyDrive/datasets/'\r\n",
        "log_directory = '/content/drive/MyDrive/{}_fulldata_otherdata/logs'\r\n",
        "output_directory = '/content/drive/MyDrive/{}_fulldata_otherdata/output/'\r\n",
        "pretrained_model = 'microsoft/codebert-base'\r\n",
        "#pretrained_model = 'microsoft/graphcodebert-base'\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "class SmellDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, encodings, labels):\r\n",
        "        self.encodings = encodings\r\n",
        "        self.labels = labels\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\r\n",
        "        return item\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.labels)\r\n",
        "\r\n",
        "class Method():\r\n",
        "    def __init__(self, text, count):\r\n",
        "        self.text = text\r\n",
        "        self.count = count\r\n",
        "\r\n",
        "def model_init():\r\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model)\r\n",
        "    model.cuda()\r\n",
        "    return model\r\n",
        "\r\n",
        "def compute_metrics(p):    \r\n",
        "    pred, labels = p\r\n",
        "    pred = np.argmax(pred, axis=1)\r\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\r\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\r\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\r\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\r\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} \r\n",
        "\r\n",
        "def hyperparam_objective(metrics):\r\n",
        "  return metrics['eval_loss']\r\n",
        "\r\n",
        "def find_hyperparams(train_data, val_data, tokenizer, smell):\r\n",
        "    training_args = TrainingArguments(\r\n",
        "        evaluation_strategy='steps',\r\n",
        "        eval_steps=250,\r\n",
        "        output_dir=output_directory.format(smell + '_hpx'),\r\n",
        "        logging_dir=log_directory.format(smell + '_hpx'),\r\n",
        "        per_device_train_batch_size=batch_size,\r\n",
        "        per_device_eval_batch_size=batch_size,\r\n",
        "        num_train_epochs=2,\r\n",
        "        warmup_steps=500,\r\n",
        "    )\r\n",
        "    \r\n",
        "    trainer = Trainer(\r\n",
        "        args=training_args,\r\n",
        "        tokenizer=tokenizer,\r\n",
        "        train_dataset=train_data, \r\n",
        "        eval_dataset=val_data, \r\n",
        "        model_init=model_init,\r\n",
        "        compute_metrics=compute_metrics,\r\n",
        "    )\r\n",
        "    return trainer.hyperparameter_search(direction='minimize', hp_space=hp_space, compute_objective=hyperparam_objective)\r\n",
        "\r\n",
        "def read_data(smell):\r\n",
        "    texts = None\r\n",
        "    labels = None\r\n",
        "    with open(data_directory + smell + '.texts', 'r') as texts_file:\r\n",
        "        texts = json.loads(texts_file.read())\r\n",
        "    with open(data_directory + smell + '.labels', 'r') as labels_file:\r\n",
        "        labels = json.loads(labels_file.read())\r\n",
        "    return texts, labels\r\n",
        "\r\n",
        "def split_data(texts, labels, smell, validation_size=0.3, max_size=60000):\r\n",
        "    positive_texts = []\r\n",
        "    negative_texts = []\r\n",
        "    negative_texts_with_block = []\r\n",
        "    positive_labels = []\r\n",
        "    negative_labels = []\r\n",
        "    negative_labels_with_block = []\r\n",
        "    bad_methods = []\r\n",
        "    negative_countx = 0\r\n",
        "\r\n",
        "    for text, label in zip(texts, labels):\r\n",
        "        if label == 1:\r\n",
        "            positive_texts.append(text)\r\n",
        "            positive_labels.append(label)\r\n",
        "        else:\r\n",
        "            stripped_text = ''.join(text.split())\r\n",
        "            if smell == 'Empty catch clause':\r\n",
        "                if 'try{' in stripped_text and ('}catch(' in stripped_text or '}catch{' in stripped_text):\r\n",
        "                    negative_texts_with_block.append(text)\r\n",
        "                    negative_labels_with_block.append(label)\r\n",
        "                else:\r\n",
        "                    negative_labels.append(text)\r\n",
        "                    negative_labels.append(label)\r\n",
        "            elif smell == 'Complex Method':\r\n",
        "                count = stripped_text.count('if(') + stripped_text.count('else') + stripped_text.count('for(') + stripped_text.count('while(') + stripped_text.count('case') + stripped_text.count('switch(') + stripped_text.count('try{') + stripped_text.count('}catch{')\r\n",
        "                bad_methods.append(Method(text, count))\r\n",
        "                negative_texts.append(text)\r\n",
        "                negative_labels.append(label)\r\n",
        "            elif smell == 'Multifaceted Abstraction':\r\n",
        "                if 5 <= (stripped_text.count('public') + stripped_text.count('private') + stripped_text.count('protected')) <= 15:\r\n",
        "                    negative_texts.append(text)\r\n",
        "                    negative_labels.append(label)\r\n",
        "            else:\r\n",
        "                negative_texts.append(text)\r\n",
        "                negative_labels.append(label)\r\n",
        "\r\n",
        "            negative_countx += 1\r\n",
        "\r\n",
        "    print('positive examples pre-trim: {}'.format(len(positive_labels)))\r\n",
        "    print('negative examples pre-trim: {}'.format(len(negative_labels)))\r\n",
        "\r\n",
        "    if smell == 'Empty catch clause':\r\n",
        "        print('negative examples with catch block pre-trim: {}'.format(len(negative_labels_with_block)))\r\n",
        "        #ensure roughly 50/50 split\r\n",
        "        negative_texts = random.sample(negative_texts, len(negative_texts_with_block)) + negative_texts_with_block\r\n",
        "        negative_labels = negative_labels[:len(negative_texts_with_block)] + negative_labels_with_block\r\n",
        "    elif smell == 'Complex Method':\r\n",
        "        sorted_method = sorted(bad_methods, key=lambda x: x.count, reverse=True)\r\n",
        "        sorted_methodx = list(reversed(sorted_method))[:len(positive_texts)]\r\n",
        "        negative_texts = sorted_method[len(positive_texts):-1 * len(positive_texts)]\r\n",
        "        negative_labels = negative_labels[:len(negative_texts)]\r\n",
        "        sorted_method = sorted_method[:len(positive_texts)]\r\n",
        "\r\n",
        "    positive_negative_ratio = len(positive_labels) / negative_countx\r\n",
        "    positive_validation_size = validation_size * positive_negative_ratio\r\n",
        "    positive_count = min(math.ceil(max_size / (1 - positive_validation_size)), len(positive_texts))\r\n",
        "    \r\n",
        "    positive_texts_training, positive_texts_validation, positive_labels_training, positive_labels_validation = train_test_split(\r\n",
        "        random.sample(positive_texts, positive_count),\r\n",
        "        positive_labels[:positive_count], \r\n",
        "        test_size=positive_validation_size)\r\n",
        "\r\n",
        "    negative_count = len(positive_texts_training) + int(len(positive_labels_validation) * math.pow(positive_negative_ratio, -1))\r\n",
        "    print(negative_count)\r\n",
        "\r\n",
        "    negative_texts_training, negative_texts_validation, negative_labels_training, negative_labels_validation = (\r\n",
        "        random.sample([t.text for t in (sorted_method + sorted_methodx)], len(positive_texts_training)),\r\n",
        "        random.sample([t.text for t in negative_texts], negative_count - len(positive_texts_training)),\r\n",
        "        [0] * len(positive_texts_training),\r\n",
        "        [0] * (negative_count - len(positive_texts_training))) if smell == 'Complex Method' else train_test_split(\r\n",
        "        random.sample(negative_texts, negative_count),\r\n",
        "        negative_labels[:negative_count],\r\n",
        "        test_size=negative_count - len(positive_texts_training))\r\n",
        "\r\n",
        "    print('positive training examples post-trim: {}'.format(len(positive_labels_training)))\r\n",
        "    print('positive validation examples post-trim: {}'.format(len(positive_labels_validation)))\r\n",
        "    print('negative training examples post-trim: {}'.format(len(negative_labels_training)))\r\n",
        "    print('negative validation examples post-trim: {}'.format(len(negative_labels_validation)))\r\n",
        "\r\n",
        "    print(type(negative_texts_training[0]))\r\n",
        "    print(type(negative_texts_validation[0]))\r\n",
        "\r\n",
        "    texts_training, labels_training = shuffle(positive_texts_training + negative_texts_training, positive_labels_training + negative_labels_training)\r\n",
        "    texts_validation, labels_validation = shuffle(positive_texts_validation + negative_texts_validation, positive_labels_validation + negative_labels_validation)\r\n",
        "    return texts_training, texts_validation, labels_training, labels_validation\r\n",
        "\r\n",
        "smells = [\r\n",
        "    'Empty catch clause',\r\n",
        "    'Complex Method', \r\n",
        "    'Multifaceted Abstraction',\r\n",
        "    'Magic Number'\r\n",
        "]\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model, use_fast=True)\r\n",
        "\r\n",
        "for smell in smells:\r\n",
        "    print('Reading data for {}...'.format(smell))\r\n",
        "    texts, labels = read_data(smell)\r\n",
        "\r\n",
        "    print('Generating splits...')\r\n",
        "\r\n",
        "    train_texts, validation_texts, train_labels, validation_labels = split_data(texts, labels, smell)\r\n",
        "\r\n",
        "    del texts\r\n",
        "    del labels\r\n",
        "\r\n",
        "    print('Tokenizing training data...')\r\n",
        "\r\n",
        "    train_encodings = tokenizer(train_texts, truncation=True)\r\n",
        "    del train_texts\r\n",
        "\r\n",
        "    print('Tokenizing validation data...')\r\n",
        "\r\n",
        "    validation_encodings = tokenizer(validation_texts, truncation=True)\r\n",
        "    del validation_texts\r\n",
        "\r\n",
        "    print('Preparing to train...')\r\n",
        "\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    train_dataset = SmellDataset(train_encodings, train_labels)\r\n",
        "    validation_dataset = SmellDataset(validation_encodings, validation_labels)\r\n",
        "\r\n",
        "    args = TrainingArguments(\r\n",
        "        evaluation_strategy='epoch',\r\n",
        "        per_device_train_batch_size=batch_size,\r\n",
        "        per_device_eval_batch_size=batch_size,\r\n",
        "        load_best_model_at_end=True,\r\n",
        "        metric_for_best_model='f1',\r\n",
        "        output_dir=output_directory.format(smell + pretrained_model),\r\n",
        "        logging_dir=log_directory.format(smell + pretrained_model),\r\n",
        "        learning_rate=5e-6,\r\n",
        "        warmup_steps=500,\r\n",
        "        num_train_epochs=5\r\n",
        "    )\r\n",
        "\r\n",
        "    trainer = Trainer(\r\n",
        "        model_init=model_init,\r\n",
        "        args=args,\r\n",
        "        train_dataset=train_dataset,\r\n",
        "        eval_dataset=validation_dataset,\r\n",
        "        tokenizer=tokenizer,\r\n",
        "        compute_metrics=compute_metrics,\r\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\r\n",
        "    )\r\n",
        "\r\n",
        "    trainer.train()\r\n",
        "\r\n",
        "    print(trainer.evaluate())\r\n",
        "\r\n",
        "    trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWwmI-BER-qE",
        "outputId": "7f1713af-e345-4a24-e029-2c756546363f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat May 22 21:13:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}